{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to evaluation the accuracy and performance of any set of LLM generated labels under `./data/labels_llm/{tag}` against the ground truth labels under `./data/labels/`.\n",
    "\n",
    "The approach here is to start with typical evaluation metrics like precision, recall, and F1 score on an overall basis and per entity category which for this project is one of `names`, `phone_numbers`, `email_addresses`, or `physical_addresses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import json\n",
    "from src.utils import get_files, clean_message\n",
    "from src.eval import create_labels_dict, merge_labels, calculate_metrics, calculate_overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "# tag we want to evaluate against the ground truth labels\n",
    "tag = \"dev_gpt4_1106_preview\"\n",
    "data_path = \"./data/emails_train_small.csv\"\n",
    "dataset = data_path.split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# lets read the training data in case we want to look at any specific message\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the labels folder: 74\n",
      "Number of files in the labels_llm/dev_gpt4_1106_preview folder: 20\n"
     ]
    }
   ],
   "source": [
    "# get a list of all files in the labels folder(s)\n",
    "\n",
    "# ground truth labels\n",
    "files_labels = get_files(f\"./data/labels/\")\n",
    "print(f\"Number of files in the labels folder: {len(files_labels)}\")\n",
    "\n",
    "# labels from the LLM\n",
    "files_labels_llm = get_files(f\"./data/labels_llm/{tag}/\")\n",
    "print(f\"Number of files in the labels_llm/{tag} folder: {len(files_labels_llm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangle data structures a little bit\n",
    "labels_llm = create_labels_dict(files_labels_llm, tag=tag)\n",
    "labels = create_labels_dict(files_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common labels: 19\n"
     ]
    }
   ],
   "source": [
    "# for all files common between the labels and labels_llm get the actual labels \n",
    "# from the files and merge them into a single dictionary that will be easier to with with\n",
    "labels_final = merge_labels(labels, labels_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message:\n",
      "\n",
      "Hi Vince\n",
      "\n",
      "\n",
      "Just wanted to thank you for your participation at POWER 2000  last week and \n",
      "for contributing to the success of the conference. The feedback we  received \n",
      "was absolutely glowing and we were delighted with the smooth-running of  the \n",
      "event. Thank you for being a key part of that. As always, your presentations  \n",
      "went down extremely well and your presence at our events makes a big \n",
      "difference,  as people are alwyas keen to hear both form you personally and \n",
      "from ENRON as a  company.\n",
      "\n",
      "As I mentioned to you, I have recently been given the  responsibility of \n",
      "creating and developing a new conference stream in the  financial technology \n",
      "sector under the Waters brand, so I would like to take this  opportunity to \n",
      "say how much I have enjoyed working with you in the past couple  of years and \n",
      "to wish you the best of luck in the future. Please stay in touch  and if you \n",
      "come to New York, please let me know so I can take you out for a  drink!\n",
      "\n",
      "Best regards and thank you again both for your patience in  helping me \n",
      "research topics and for being so willing to participate at our  events.\n",
      "Emma\n",
      "?\n",
      "?\n",
      "Emma Wolfin\n",
      "Manager, Waters  conferences\n",
      "Tel 212 925 1864 ext 151\n",
      "\n",
      "Ground truth labels:\n",
      "\n",
      "{'email_addresses': [''],\n",
      " 'names': ['Emma Wolfin'],\n",
      " 'phone_numbers': ['212 925 1864 ext 151'],\n",
      " 'physical_addresses': ['']}\n",
      "\n",
      "LLM generated labels:\n",
      "\n",
      "{'email_addresses': [],\n",
      " 'names': ['Vince', 'Emma', 'Emma Wolfin'],\n",
      " 'phone_numbers': ['212 925 1864'],\n",
      " 'physical_addresses': []}\n"
     ]
    }
   ],
   "source": [
    "# lets look at an example of what we have\n",
    "file = 'kaminski-v/all_documents/9240.'\n",
    "\n",
    "# lets get raw message cleaned in our standardized way\n",
    "print(\"Message:\\n\")\n",
    "print(clean_message(df.query(\"file == @file\").message.values[0]))\n",
    "\n",
    "# lets look at the ground truth labels\n",
    "print(\"\\nGround truth labels:\\n\")\n",
    "pp.pprint(labels_final[file]['labels'])\n",
    "\n",
    "# lets look at the predicted labels from the LLM\n",
    "print(\"\\nLLM generated labels:\\n\")\n",
    "pp.pprint(labels_final[file]['predicted_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "entity_metrics = calculate_metrics(labels_final)\n",
    "overall_metrics = calculate_overall_metrics(entity_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'emails_train_small',\n",
      " 'entity_metrics': {'email_addresses': {'F1 Score': 0.2941,\n",
      "                                        'FN': 21,\n",
      "                                        'FP': 27,\n",
      "                                        'Precision': 0.2703,\n",
      "                                        'Recall': 0.3226,\n",
      "                                        'TP': 10},\n",
      "                    'names': {'F1 Score': 0.619,\n",
      "                              'FN': 23,\n",
      "                              'FP': 105,\n",
      "                              'Precision': 0.4976,\n",
      "                              'Recall': 0.8189,\n",
      "                              'TP': 104},\n",
      "                    'phone_numbers': {'F1 Score': 0.5294,\n",
      "                                      'FN': 15,\n",
      "                                      'FP': 1,\n",
      "                                      'Precision': 0.9,\n",
      "                                      'Recall': 0.375,\n",
      "                                      'TP': 9},\n",
      "                    'physical_addresses': {'F1 Score': 0.1379,\n",
      "                                           'FN': 18,\n",
      "                                           'FP': 7,\n",
      "                                           'Precision': 0.2222,\n",
      "                                           'Recall': 0.1,\n",
      "                                           'TP': 2}},\n",
      " 'overall_metrics': {'Overall F1 Score': 0.5353,\n",
      "                     'Overall Precision': 0.4717,\n",
      "                     'Overall Recall': 0.6188}}\n"
     ]
    }
   ],
   "source": [
    "# save the metrics to a json file\n",
    "metrics = {\n",
    "    \"dataset\": dataset,\n",
    "    \"entity_metrics\": entity_metrics,\n",
    "    \"overall_metrics\": overall_metrics\n",
    "}\n",
    "pp.pprint(metrics)\n",
    "with open(f\"./data/labels_llm/{tag}/evaluation_metrics_{dataset}.json\", \"w\") as f:\n",
    "    json.dump(metrics, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
